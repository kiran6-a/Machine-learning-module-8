{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Information Gain (IG) measures how much “information” (or reduction in impurity) a feature gives when used to split data in a Decision Tree.\n",
        "It tells us how well a feature separates the classes.\n",
        "\n",
        "Formula:\n",
        "𝐼\n",
        "𝐺\n",
        "(\n",
        "𝐷\n",
        ",\n",
        "𝐴\n",
        ")\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑣\n",
        "∈\n",
        "𝑉\n",
        "𝑎\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "𝑠\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "∣\n",
        "𝐷\n",
        "𝑣\n",
        "∣\n",
        "∣\n",
        "𝐷\n",
        "∣\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        "𝑣\n",
        ")\n",
        "IG(D,A)=Entropy(D)−\n",
        "v∈Values(A)\n",
        "∑\n",
        "\t​\n",
        "\n",
        "∣D∣\n",
        "∣D\n",
        "v\n",
        "\t​\n",
        "\n",
        "∣\n",
        "\t​\n",
        "\n",
        "×Entropy(D\n",
        "v\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "𝐷\n",
        "D → dataset\n",
        "\n",
        "𝐴\n",
        "A → attribute (feature)\n",
        "\n",
        "𝐷\n",
        "𝑣\n",
        "D\n",
        "v\n",
        "\t​\n",
        "\n",
        " → subset of data where feature\n",
        "𝐴\n",
        "=\n",
        "𝑣\n",
        "A=v\n",
        "\n",
        "How it works:\n",
        "\n",
        "Calculate the entropy of the dataset (overall impurity).\n",
        "\n",
        "For each feature, compute the entropy after splitting on that feature.\n",
        "\n",
        "Information Gain = reduction in entropy.\n",
        "\n",
        "Choose the feature with maximum Information Gain for the split.\n",
        "\n",
        "Example:\n",
        "\n",
        "If splitting on “Age” reduces entropy from 0.9 → 0.3,\n",
        "then\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "0.9\n",
        "−\n",
        "0.3\n",
        "=\n",
        "0.6\n",
        "IG=0.9−0.3=0.6.\n",
        "This means “Age” is a strong predictor.\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Feature\tGini Impurity\tEntropy\n",
        "Formula\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑p\n",
        "i\n",
        "2\n",
        "\t​\n",
        "\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−∑p\n",
        "i\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "(p\n",
        "i\n",
        "\t​\n",
        "\n",
        ")\n",
        "Range\t0 (pure) to 0.5 (binary mix)\t0 (pure) to 1 (binary mix)\n",
        "Interpretation\tMeasures misclassification probability\tMeasures information (bits) needed to classify\n",
        "Computation Speed\tFaster\tSlightly slower (uses log)\n",
        "Used In\tCART (Classification and Regression Trees)\tID3, C4.5 algorithms\n",
        "When to Use\tFor faster training, with large datasets\tWhen interpretability or information theory is key\n",
        "\n",
        "Summary:\n",
        "Both measure impurity. Gini is simpler and faster; Entropy is more informative but computationally heavier.\n",
        "\n",
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-Pruning (also called Early Stopping) is the technique of stopping tree growth early—before it becomes too complex.\n",
        "\n",
        "How it works:\n",
        "\n",
        "You specify stopping criteria such as:\n",
        "\n",
        "Maximum depth (max_depth)\n",
        "\n",
        "Minimum samples to split (min_samples_split)\n",
        "\n",
        "Minimum information gain threshold\n",
        "\n",
        "If further splits don’t improve accuracy or gain enough information, tree growth stops.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Prevents overfitting early.\n",
        "\n",
        "Faster training and simpler model.\n",
        "\n",
        "Example:\n",
        "\n",
        "DecisionTreeClassifier(max_depth=4)\n",
        "→ Stops tree at depth 4 even if more splits possible.\n",
        "\n",
        "Question 4: Python Program – Decision Tree Classifier (Gini Impurity)\n",
        "\n",
        "Answer:\n",
        "\n",
        "# Decision Tree using Gini Impurity\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "# Display feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': clf.feature_importances_\n",
        "})\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", round(accuracy, 3))\n",
        "print(\"\\nFeature Importances:\\n\", feature_importances)\n",
        "\n",
        "\n",
        "✅ Output (Example):\n",
        "\n",
        "Decision Tree Accuracy: 0.977\n",
        "\n",
        "Feature Importances:\n",
        "           Feature  Importance\n",
        "0  sepal length (cm)   0.015\n",
        "1   sepal width (cm)   0.025\n",
        "2  petal length (cm)   0.555\n",
        "3   petal width (cm)   0.405\n",
        "\n",
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.\n",
        "\n",
        "It finds the best separating hyperplane that divides data into classes with maximum margin.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "Support Vectors: Data points closest to the boundary.\n",
        "\n",
        "Margin: Distance between the hyperplane and nearest data points.\n",
        "\n",
        "Goal: Maximize margin for better generalization.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Works well for high-dimensional data.\n",
        "\n",
        "Effective when classes are separable.\n",
        "\n",
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Kernel Trick allows SVM to perform classification on non-linear data by mapping it into a higher-dimensional space — without explicitly computing the transformation.\n",
        "\n",
        "Common Kernels:\n",
        "Kernel\tFunction\tUse Case\n",
        "Linear\n",
        "𝑥\n",
        "⋅\n",
        "𝑦\n",
        "x⋅y\tLinearly separable data\n",
        "Polynomial\n",
        "(\n",
        "𝑥\n",
        "⋅\n",
        "𝑦\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "(x⋅y+c)\n",
        "d\n",
        "\tCurved boundaries\n",
        "RBF (Gaussian)\t( \\exp(-\\gamma\n",
        "\n",
        "In short:\n",
        "Kernels help SVM handle complex patterns that are not linearly separable.\n",
        "\n",
        "Question 7: Python Program – SVM with Linear and RBF Kernels (Wine Dataset)\n",
        "\n",
        "Answer:\n",
        "\n",
        "# SVM with Linear and RBF kernels\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(\"SVM Linear Kernel Accuracy:\", round(linear_acc, 3))\n",
        "print(\"SVM RBF Kernel Accuracy:\", round(rbf_acc, 3))\n",
        "\n",
        "\n",
        "✅ Output (Example):\n",
        "\n",
        "SVM Linear Kernel Accuracy: 0.981\n",
        "SVM RBF Kernel Accuracy: 0.963\n",
        "\n",
        "\n",
        "Conclusion:\n",
        "Linear kernel works slightly better for this dataset (data is nearly linearly separable).\n",
        "\n",
        "Question 8: What is the Naïve Bayes Classifier, and why is it called “Naïve”?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem, assuming features are independent.\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(C∣X)=\n",
        "P(X)\n",
        "P(X∣C)P(C)\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(C∣X): probability of class\n",
        "𝐶\n",
        "C given features\n",
        "𝑋\n",
        "X\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C): likelihood\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "P(C): prior probability\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X): evidence\n",
        "\n",
        "Why “Naïve”?\n",
        "\n",
        "It assumes all features are independent, which is rarely true in real-world data — but it still works surprisingly well!\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Very fast, works well with large datasets.\n",
        "\n",
        "Performs well for text and spam filtering.\n",
        "\n",
        "Question 9: Difference Between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "Type\tUsed For\tData Type\tExample Use Case\n",
        "GaussianNB\tContinuous features\tReal-valued (height, weight)\tMedical data, sensors\n",
        "MultinomialNB\tDiscrete counts\tWord frequencies, counts\tText classification\n",
        "BernoulliNB\tBinary features\t0/1 features (present/absent)\tSpam detection (word present/absent)\n",
        "Question 10: Python Program – Gaussian Naïve Bayes on Breast Cancer Dataset\n",
        "\n",
        "Answer:\n",
        "\n",
        "# Gaussian Naive Bayes on Breast Cancer dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", round(accuracy, 3))\n",
        "\n",
        "\n",
        "✅ Output (Example):\n",
        "\n",
        "Gaussian Naive Bayes Accuracy: 0.953"
      ],
      "metadata": {
        "id": "A_c_HqKnVl-G"
      }
    }
  ]
}